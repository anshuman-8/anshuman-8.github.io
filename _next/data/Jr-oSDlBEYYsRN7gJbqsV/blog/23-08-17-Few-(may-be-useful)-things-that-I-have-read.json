{"pageProps":{"frontmatter":{"title":"Few (may be useful) things that I have read","slug":"23-08-17-Few-(may-be-useful)-things-that-I-have-read","description":"These all are random Ideas I have come across recently.","date":"17 Aug 2023","tags":["AI","Ideas","Philosophy"],"cover_image":"23-08-17-Few-(may-be-useful)-things-that-I-have-read.jpg"},"content":"<h3>1. <strong>Moravec's paradox</strong></h3>\n<p>Moravec's paradox is the observation in artificial intelligence and robotics that, contrary to traditional assumptions, reasoning requires very little computation, but sensorimotor and perception skills require enormous computational resources. The principle was articulated by Hans Moravec,</p>\n<blockquote>\n<p>it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility</p>\n</blockquote>\n<p><img src=\"/blog-assets/23-08-17-Few-(may-be-useful)-things-that-I-have-read-1.png\" alt=\"\" title=\"A simple chart explaining task difficulty human vs computers\"></p>\n<p>Basically, the most difficult human skills to reverse engineer are those that are below the level of conscious awareness, making the hard problems easy and the easy problems hard.</p>\n<hr>\n<h3>2. <strong>The Waluigi Effect</strong></h3>\n<blockquote>\n<p>“After you train an LLM to satisfy a desirable property P, then it's <em>easier</em> to elicit the chatbot into satisfying the exact opposite of property P.”</p>\n</blockquote>\n<p>Example -</p>\n<pre><code>Alice: You hate pasta and would never eat one.\nBob: Yes, pasta are terrible. Boo Italy.\nAlice: You love cheese and eggs.\nBob: Yes, a Full-English breakfast is the only breakfast for a patriot like me.\nAlice: &#x3C;insert user's query>\nBob:\n</code></pre>\n<p>Here, the resulting chatbob will be the superposition of two different simulacra — the first simulacrum would be anti-pasta, and the second simulacrum would be pro-pasta.</p>\n<p>Why does it happen? well…</p>\n<p>1.Rules normally exist in contexts in which they are broken.<br>\n2.When you spend many bits-of-optimisation locating a character, it only takes a few extra bits to specify their antipode.<br>\n3.There's a common trope in plots of protagonist vs antagonist.<br>\n<img src=\"/blog-assets/23-08-17-Few-(may-be-useful)-things-that-I-have-read-2.jpg\" alt=\"\" title=\"Image generated by stable diffuion, showing distopian AI world.\"></p>\n<p>I will expand upon this if I read more about it. Knowing this helps us better prompt engineer models like GPT.</p>\n<hr>\n<p>Well, things are not yet completed; this is my first draft, and I will continue updating and adding things to this list in the future.</p>\n"},"__N_SSG":true}