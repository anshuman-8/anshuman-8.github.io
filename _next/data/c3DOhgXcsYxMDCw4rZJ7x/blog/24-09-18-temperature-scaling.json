{"pageProps":{"frontmatter":{"title":"What is temperature scaling?","slug":"24-09-18-temperature-scaling","description":"Basic understanding and implimentaion of temperature scaling in machine learning","date":"18 Sep 2024","tags":["ML","Deep Learning","Post-hoc"],"cover_image":"24-09-18-temperature-scaling/temp-scale-banner.png"},"content":"<h3>What is temperature scaling (in short)</h3>\n<p>Temperature scaling is a post-processing technique to make neural networks calibrated. After temperature scaling, you can trust the probabilities output by a neural network. The model’s prediction probability is calibrated by scaling using the temperature value.</p>\n<h3>What is Calibration in machine learning</h3>\n<p>A model is said to be perfectly calibrated if the predicted probabilities of outcomes align closely with the actual outcomes.</p>\n<p>For instance, if a model predicts an event with a 70% probability, then ideally, out of 100 such predictions, approximately 70 should result in the event actually occurring. The probability associated with the predicted class label should reflect its ground truth correctness likelihood.</p>\n<p><img src=\"/blog-assets/24-09-18-temperature-scaling/reliablity-curve.png\" alt=\"Reliablity-Curve\"></p>\n<p>Fig 1 A and B</p>\n<p>However, many experiments have revealed that modern neural networks are no longer well-calibrated. Modern deep learning models are usually overly confident in their predictions. Such overconfidence can be problematic, especially in applications where the predicted probabilities are used to make critical decisions.  In the figure 1 the left one align with the accuracy of the model across all confidence level and the right one can’t. Some of the samples appear to have high confidence between 0.8 and 0.9 but the accuracy is just about 0.5. This means that the model confidence means nothing to tell about how is its performance.</p>\n<p>The above Plot is called <strong>Reliability</strong> curve. If the model is perfectly calibrated, the points on the curve will fall along the diagonal line (y = x). Points above the diagonal indicate underconfidence, while points below indicate overconfidence. For example in the below plot we can see an overconfident model where the predictions are far below the diagonal line. This indicates that the model assigns high confidence to predictions even when they are incorrect.</p>\n<p><img src=\"/blog-assets/24-09-18-temperature-scaling/reliablity-curve-2.png\" alt=\"Reliablity-Curve-2\"></p>\n<p>Fig 2</p>\n<p>Here is an amazing blog explaining model calibration - <a href=\"https://pair.withgoogle.com/explorables/uncertainty-calibration/\">Are Model Predictions Probabilities? - By PAIR</a></p>\n<h3>How temperature scaling works</h3>\n<p>In classification problem the model output (logits) before passing through the softmax is scaled, and then passed through the softmax to give model probabilities.</p>\n<p><img src=\"/blog-assets/24-09-18-temperature-scaling/temp-formula.png\" alt=\"Temperature scaling formula\"></p>\n<p>In above formula, Pi is the probability of that class, zj is the logit and T is the temperature value.</p>\n<p>Temperature scaling uses a single scalar parameter <em>T</em> > 0, where <em>T</em> is the temperature, to rescale logit scores before applying the softmax function, as shown in the following figure. Because the same <em>T</em> is used for all classes, the softmax output with scaling has a monotonic relationship with unscaled output. In overconfident models where <em>T</em> > 1, the recalibrated probabilities have a lower value than the original probabilities, and they are more evenly distributed between 0 and 1. When <em>T</em> = 1, you recover the original probability with the default softmax function.</p>\n<p>In simple terms, temperature scaling adjusts how confident a model is about its predictions. If a model is overconfident (i.e., predicting high probabilities for wrong predictions), temperature scaling ‘softens’ these predictions. By adjusting a temperature parameter (T), we can reduce the confidence of overly confident predictions without changing the model’s underlying structure.</p>\n<h3>Benefits of temperature scaling</h3>\n<p>As discussed above it helps us calibrate the model and makes the model probabilities more reliable. In high-stakes environments where decisions based on these predictions can affect human lives, financial stability, or critical infrastructure (example: healthcare and autonomous driving). The probability associated with the predicted class label should reflect its ground truth correctness likelihood. Good confidence estimates provide a valuable extra bit of information to establish trustworthiness on the model.</p>\n<h3>Code</h3>\n<p>To implement temperature scaling, we need to adjust the logits of our neural network outputs before applying softmax. The following Python class shows how we train the temperature parameter to improve calibration:</p>\n<p>Below is the class to to train the temperature parameter for a model</p>\n<pre><code>class TemperatureScaling(nn.Module):\n    def __init__(self):\n        super(TemperatureScaling, self).__init__()\n        self.temperature = nn.Parameter(torch.ones(1) * 1.5 )  # Initialize temperature parameter\n\n    def forward(self, logits):\n        # Scale logits by temperature\n        return self.temperature_scale(logits)\n    \n    def temperature_scale(self, logits):\n        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n        return logits / temperature\n</code></pre>\n<p>Load the pre-trained model from its checkpoint</p>\n<pre><code>inference_model = # load_from_checkpoint(model_checkpoint)\ntemp_scaling = TemperatureScaling()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.LBFGS([temp_scaling.temperature], lr=0.05, max_iter=60)\n</code></pre>\n<p>The optimize_temperature function uses the validation set of the dataset to train the temperature parameter.</p>\n<pre><code>def optimize_temperature(inference_model, temp_scaling, val_loader, optimizer, criterion, device):\n    inference_model.to(device).eval()  # Ensure the model is in evaluation mode\n    temp_scaling.to(device).train()  # Set temperature scaling layer to training mode for optimization\n    def closure():\n        optimizer.zero_grad()\n        losses = []\n\n        for batch in tqdm(val_loader):\n            images, labels, paths, patient_name, features = batch\n            inputs = images.to(device)\n            labels = labels.to(device)\n            feature = features.to(device)\n            with torch.no_grad():\n                logits = inference_model(inputs, feature)  # Get logits from the model\n            scaled_logits = temp_scaling(logits)  # Scale logits using the temperature layer\n            loss = criterion(scaled_logits, labels)  # Calculate loss\n            losses.append(loss.item())\n            loss.backward()  # Backpropagate to update temperature\n        temp_value = temp_scaling.temperature.detach().cpu().numpy()\n        return sum(losses) / len(losses)\n    optimizer.step(closure)\n</code></pre>\n"},"__N_SSG":true}